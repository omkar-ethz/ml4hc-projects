{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFa6pg60M1Z"
      },
      "source": [
        "# Check GPU and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_5WRJRx0TMO",
        "outputId": "a0c4be9d-ca03-4fe4-e152-741ff8d1ea9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mon May 15 16:34:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjoKnMLcz8Q6"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8uwY6Ruz8RA"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVLIsKMUz8RE",
        "outputId": "8d8c0455-0ffb-4b04-b112-f1f65e3113f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-13471585cd61>:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  tweets = pd.read_csv('/content/gdrive/MyDrive/TweetsCOV19.csv')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "tweets = pd.read_csv('/content/gdrive/MyDrive/TweetsCOV19.csv')\n",
        "tweets.rename(columns={\"TweetText\": \"x\"}, inplace=True)\n",
        "tweets['x'] = tweets['x'].astype(str)\n",
        "tweets.dropna(subset=[\"Sentiment\"], inplace=True)\n",
        "tweets['UserLocation'] = tweets['UserLocation'].fillna(\"unknown\")\n",
        "tweets[['pos','neg']] = tweets['Sentiment'].str.split(\" \", expand=True)\n",
        "tweets[\"strat\"] = tweets['pos'].astype(int) + tweets['neg'].astype(int) #+\";\"+ pd.to_datetime(tweets[\"Timestamp\"]).dt.month.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX6fQKnmz8RH"
      },
      "source": [
        "## Preprocessing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doA80VWCz8RL"
      },
      "outputs": [],
      "source": [
        "emoticons = {\n",
        "    ':*': '<kiss>',\n",
        "    ':-*': '<kiss>',\n",
        "    ':x': '<kiss>',\n",
        "    ':-)': '<happy>',\n",
        "    ':-))': '<happy>',\n",
        "    ':-)))': '<happy>',\n",
        "    ':-))))': '<happy>',\n",
        "    ':-)))))': '<happy>',\n",
        "    ':-))))))': '<happy>',\n",
        "    ':)': '<happy>',\n",
        "    ':))': '<happy>',\n",
        "    ':)))': '<happy>',\n",
        "    ':))))': '<happy>',\n",
        "    ':)))))': '<happy>',\n",
        "    ':))))))': '<happy>',\n",
        "    ':)))))))': '<happy>',\n",
        "    ':o)': '<happy>',\n",
        "    ':]': '<happy>',\n",
        "    ':3': '<happy>',\n",
        "    ':c)': '<happy>',\n",
        "    ':>': '<happy>',\n",
        "    '=]': '<happy>',\n",
        "    '8)': '<happy>',\n",
        "    '=)': '<happy>',\n",
        "    ':}': '<happy>',\n",
        "    ':^)': '<happy>',\n",
        "    '|;-)': '<happy>',\n",
        "    \":'-)\": '<happy>',\n",
        "    \":')\": '<happy>',\n",
        "    '\\\\o/': '<happy>',\n",
        "    '\\\\0/': '<happy>',\n",
        "    ':-d': '<laugh>',\n",
        "    ':d': '<laugh>',\n",
        "    '8-d': '<laugh>',\n",
        "    '8d': '<laugh>',\n",
        "    'x-d': '<laugh>',\n",
        "    'xd': '<laugh>',\n",
        "    '=-d': '<laugh>',\n",
        "    '=D': '<laugh>',\n",
        "    '=-3': '<laugh>',\n",
        "    '=3': '<laugh>',\n",
        "    'b^d': '<laugh>',\n",
        "    '>:[': '<sad>',\n",
        "    ':-(': '<sad>',\n",
        "    ':-((': '<sad>',\n",
        "    ':-(((': '<sad>',\n",
        "    ':-((((': '<sad>',\n",
        "    ':-(((((': '<sad>',\n",
        "    ':-((((((': '<sad>',\n",
        "    ':-(((((((': '<sad>',\n",
        "    ':(': '<sad>',\n",
        "    ':((': '<sad>',\n",
        "    ':(((': '<sad>',\n",
        "    ':((((': '<sad>',\n",
        "    ':(((((': '<sad>',\n",
        "    ':((((((': '<sad>',\n",
        "    ':(((((((': '<sad>',\n",
        "    ':((((((((': '<sad>',\n",
        "    ':-c': '<sad>',\n",
        "    ':c': '<sad>',\n",
        "    ':-<': '<sad>',\n",
        "    ':<': '<sad>',\n",
        "    ':-[': '<sad>',\n",
        "    ':[': '<sad>',\n",
        "    ':{': '<sad>',\n",
        "    ':-||': '<sad>',\n",
        "    ':@': '<sad>',\n",
        "    \":'-(\": '<sad>',\n",
        "    \":'(\": '<sad>',\n",
        "    'd:<': '<sad>',\n",
        "    'd:': '<sad>',\n",
        "    'd8': '<sad>',\n",
        "    'd;': '<sad>',\n",
        "    'd=': '<sad>',\n",
        "    'dX': '<sad>',\n",
        "    'v.v': '<sad>',\n",
        "    \"d-':\": '<sad>',\n",
        "    '(>_<)': '<sad>',\n",
        "    ':|': '<sad>',\n",
        "    '>:O': '<surprise>',\n",
        "    ':-O': '<surprise>',\n",
        "    ':-o': '<surprise>',\n",
        "    ':O': '<surprise>',\n",
        "    '°o°': '<surprise>',\n",
        "    'o_O': '<surprise>',\n",
        "    'o_0': '<surprise>',\n",
        "    'o.O': '<surprise>',\n",
        "    'o-o': '<surprise>',\n",
        "    '8-0': '<surprise>',\n",
        "    '|-O': '<surprise>',\n",
        "    ';-)': '<wink>',\n",
        "    ';)': '<wink>',\n",
        "    '*-)': '<wink>',\n",
        "    '*)': '<wink>',\n",
        "    ';-]': '<wink>',\n",
        "    ';]': '<wink>',\n",
        "    ';d': '<wink>',\n",
        "    ';^)': '<wink>',\n",
        "    ':-,': '<wink>',\n",
        "    '>:p': '<tong>',\n",
        "    ':-p': '<tong>',\n",
        "    ':p': '<tong>',\n",
        "    'x-': '<tong>',\n",
        "    'x-p': '<tong>',\n",
        "    'xp': '<tong>',\n",
        "    ':-p': '<tong>',\n",
        "    ':p': '<tong>',\n",
        "    '=p': '<tong>',\n",
        "    ':-Þ': '<tong>',\n",
        "    ':Þ': '<tong>',\n",
        "    ':-b': '<tong>',\n",
        "    ':b': '<tong>',\n",
        "    ':-&': '<tong>',\n",
        "    '>:\\\\': '<annoyed>',\n",
        "    '>:/': '<annoyed>',\n",
        "    ':-/': '<annoyed>',\n",
        "    ':-.': '<annoyed>',\n",
        "    ':/': '<annoyed>',\n",
        "    ':\\\\': '<annoyed>',\n",
        "    '=/': '<annoyed>',\n",
        "    '=\\\\': '<annoyed>',\n",
        "    ':L': '<annoyed>',\n",
        "    '=L': '<annoyed>',\n",
        "    ':S': '<annoyed>',\n",
        "    '>.<': '<annoyed>',\n",
        "    ':-|': '<annoyed>',\n",
        "    '<:-|': '<annoyed>',\n",
        "    ':-x': '<seallips>',\n",
        "    ':x': '<seallips>',\n",
        "    ':-#': '<seallips>',\n",
        "    ':#': '<seallips>',\n",
        "    'o:-)': '<angel>',\n",
        "    '0:-3': '<angel>',\n",
        "    '0:3': '<angel>',\n",
        "    '0:-)': '<angel>',\n",
        "    '0:)': '<angel>',\n",
        "    '0;^)': '<angel>',\n",
        "    '>:)': '<devil>',\n",
        "    '>:d': '<devil>',\n",
        "    '>:-d': '<devil>',\n",
        "    '>;)': '<devil>',\n",
        "    '>:-)': '<devil>',\n",
        "    '}:-)': '<devil>',\n",
        "    '}:)': '<devil>',\n",
        "    '3:-)': '<devil>',\n",
        "    '3:)': '<devil>',\n",
        "    'o/\\\\o': '<highfive>',\n",
        "    '^5': '<highfive>',\n",
        "    '>_>^': '<highfive>',\n",
        "    '^<_<': '<highfive>',\n",
        "    '<3': '<heart>'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83MXA1qBz8RT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def _load_noslang_data():\n",
        "    noslang_dict = {}\n",
        "    infile = open(\"/content/gdrive/MyDrive/noslang_mod.txt\", 'r')\n",
        "    for line in infile:\n",
        "        items = line.split(' - ')\n",
        "        if len(items[0]) > 0 and len(items) > 1:\n",
        "            noslang_dict[items[0].strip()] = items[1].strip()\n",
        "    return noslang_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UshC8Mz2z8RW",
        "outputId": "e0cf02d0-ec2e-4933-d906-7cd662f99d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, Optional\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "def to_lower(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains sentences.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda sentence: sentence.lower())\n",
        "\n",
        "def remove_punctuation(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains sentences.\n",
        "  \"\"\"\n",
        "\n",
        "  df[x_col] =df[x_col].apply(lambda sentence: re.sub(r'(?<=\\w)[^\\s\\w](?![^\\s])', '', sentence ) ) \n",
        "\n",
        "\n",
        "\n",
        "def replace_abbreviations(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  slang = _load_noslang_data()\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [slang.get(token, token) for token in tokens])\n",
        "\n",
        "def simplify_haha(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains sentences\n",
        "  \"\"\"\n",
        "  haha = r\"\\ba?h+a+\\-?h+a+\\-?[h+a+\\-?]*\\b\"\n",
        "  df[x_col] = df[x_col].apply(lambda sentence: re.sub(haha, 'haha', sentence))\n",
        "\n",
        "\n",
        "def replace_emoticons(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [emoticons.get(token, token) for token in tokens])\n",
        "\n",
        "  \n",
        "def tokenize(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains sentences.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda sentence: tokenizer.tokenize(sentence))\n",
        "\n",
        "def remove_tag_tokens(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [w for w in tokens if not w in ['user', '<url>']])\n",
        "\n",
        "def remove_stopwords(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [w for w in tokens if not w in stop_words])\n",
        "\n",
        "def lemmatize(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
        "\n",
        "def remove_single_symbols(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [w for w in tokens if len(w) > 1])\n",
        "\n",
        "def spelling_correction(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].progress_apply(lambda tokens: [Word(w).correct() for w in tokens])\n",
        "\n",
        "\n",
        "def replace_user_handles(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [w if not (w.startswith(\"@\") and len(w) > 1) else \"<user>\" for w in tokens])\n",
        "\n",
        "def replace_urls(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: [w if not (w.startswith(\"http://\") or w.startswith(\"https://\") or w.startswith(\"www.\")) else \"<url>\" for w in tokens])\n",
        "\n",
        "def untokenize(df: pd.DataFrame, x_col='x'):\n",
        "  \"\"\"\n",
        "  To be applied to a dataframe with a column called 'x' that contains tokens.\n",
        "  \"\"\"\n",
        "  df[x_col] = df[x_col].apply(lambda tokens: \" \".join(tokens))\n",
        "\n",
        "def preprocess(df: pd.DataFrame, flags: Optional[Dict[str, bool]], x_col='x'):\n",
        "  if flags is not None:\n",
        "    if flags.get('to_lower', False):\n",
        "      to_lower(df, x_col=x_col)\n",
        "    if flags.get('remove_punctuation', False):\n",
        "      remove_punctuation(df, x_col=x_col)\n",
        "    if flags.get('simplify_haha', False):\n",
        "      simplify_haha(df, x_col=x_col)\n",
        "    if flags.get('tokenize', False):\n",
        "      tokenize(df, x_col=x_col)\n",
        "    if flags.get('replace_abbreviations', False):\n",
        "      replace_abbreviations(df, x_col=x_col)\n",
        "    if flags.get('replace_emoticons', False):\n",
        "      replace_emoticons(df, x_col=x_col)\n",
        "    if flags.get('replace_user_handles', False):\n",
        "      replace_user_handles(df, x_col=x_col)\n",
        "    if flags.get('replace_urls', False):\n",
        "      replace_urls(df, x_col=x_col)  \n",
        "    if flags.get('remove_tag_tokens', False):\n",
        "      remove_tag_tokens(df, x_col=x_col)\n",
        "    if flags.get('remove_stopwords', False):\n",
        "      remove_stopwords(df, x_col=x_col)\n",
        "    if flags.get('lemmatize', False):\n",
        "      lemmatize(df, x_col=x_col)\n",
        "    if flags.get('remove_single_symbols', False):\n",
        "      remove_single_symbols(df, x_col=x_col)\n",
        "    if flags.get('spelling_correction', False):\n",
        "      spelling_correction(df, x_col=x_col)\n",
        "    if flags.get('untokenize', False):\n",
        "      untokenize(df, x_col=x_col)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4thPfbSKz8RZ"
      },
      "source": [
        "### Processing tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ8eQY_az8Rc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "unprocessed_tweets = copy.deepcopy(tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgnXZDd1z8Rf"
      },
      "outputs": [],
      "source": [
        "preprocess(tweets, flags={'to_lower': True, 'remove_punctuation': True, 'simplify_haha': True, 'tokenize': True, 'replace_abbreviations': True, 'replace_emoticons': True, 'replace_user_handles': True, 'remove_stopwords': True, 'lemmatize': True, 'remove_single_symbols': True, 'spelling_correction': False, 'replace_urls': True, 'untokenize':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qstPsbk4z8Rg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_data(tweets):\n",
        "    train_tweets, test_tweets, train_pos, test_pos, train_neg, test_neg, _, test_strat = train_test_split(tweets['x'], tweets['pos'], tweets['neg'], tweets['strat'], test_size=0.2, random_state=0, stratify=tweets[['strat']])\n",
        "    val_tweets, test_tweets, val_pos, test_pos, val_neg, test_neg = train_test_split(test_tweets, test_pos, test_neg, test_size=0.5, random_state=0, stratify=test_strat)\n",
        "    return train_tweets, val_tweets, test_tweets, train_pos, val_pos, test_pos, train_neg, val_neg, test_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCqeGebHz8Ri"
      },
      "outputs": [],
      "source": [
        "train_tweets, val_tweets, test_tweets, train_pos, val_pos, test_pos, train_neg, val_neg, test_neg = split_data(tweets)\n",
        "\n",
        "\n",
        "# Reset the indices\n",
        "train_tweets = train_tweets.reset_index(drop=True)\n",
        "train_pos = train_pos.reset_index(drop=True)\n",
        "train_neg = train_neg.reset_index(drop=True)\n",
        "val_tweets = val_tweets.reset_index(drop=True)\n",
        "val_pos = val_pos.reset_index(drop=True)\n",
        "val_neg = val_neg.reset_index(drop=True)\n",
        "test_tweets = test_tweets.reset_index(drop=True)\n",
        "test_pos = test_pos.reset_index(drop=True)\n",
        "test_neg = test_neg.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAzTs4DN3at2",
        "outputId": "4a7b3b3e-3258-4c79-e9d5-921afcf67999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "0         -1\n",
            "1         -2\n",
            "2         -1\n",
            "3         -1\n",
            "4         -1\n",
            "          ..\n",
            "540291    -1\n",
            "540292    -3\n",
            "540293    -4\n",
            "540294    -1\n",
            "540295    -1\n",
            "Name: neg, Length: 540296, dtype: object\n",
            "<class 'numpy.int64'>\n",
            "0        -1\n",
            "1        -2\n",
            "2        -1\n",
            "3        -1\n",
            "4        -1\n",
            "         ..\n",
            "540291   -1\n",
            "540292   -3\n",
            "540293   -4\n",
            "540294   -1\n",
            "540295   -1\n",
            "Name: neg, Length: 540296, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#print(train_tweets)\n",
        "print(type(train_neg[1]))\n",
        "print(train_neg)\n",
        "train_neg = train_neg.astype(int)\n",
        "train_pos = train_pos.astype(int)\n",
        "val_neg = val_neg.astype(int)\n",
        "val_pos = val_pos.astype(int)\n",
        "print(type(train_neg[1]))\n",
        "print(train_neg)\n",
        "#print(train_pos)\n",
        "\n",
        "#print(tweets.iloc[393888])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHpUC0v5z8Rj",
        "outputId": "a91b8011-54f6-46dc-811d-194532e215f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(540296, 540296, 67537, 67537)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_tweets), len(train_pos), len(val_tweets), len(test_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9WQd0Nkk1Ta",
        "outputId": "2fda0759-2d4b-4444-f464-e5c8db2e6a1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                      need stripper today’s d&amp;d sesh!!\n",
              "1         intention pointing (1 administration criticize...\n",
              "2         #exclusive video youtube tablighi jamaat chief...\n",
              "3         <user> cover tried echo art nicholas grunas \"t...\n",
              "4         florida’s hepatitis outbreak prompt vaccine pu...\n",
              "                                ...                        \n",
              "540291    acting chief executive china's #hongkong speci...\n",
              "540292    <user> human right start decent life governmen...\n",
              "540293    <user> yes let's take post joke \"hey italian g...\n",
              "540294    {{google say it’s building nationwide coronavi...\n",
              "540295    <user> maybe opened big store social distancin...\n",
              "Name: x, Length: 540296, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk5Cf2Oz8Rm"
      },
      "source": [
        "# Part 3: Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o04x_rmkz8Ro"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language representational model that pre-trains representations from unlabeled text by jointly conditioning on left and right context in all layers. These representations are not task-specific, such that the model can be pre-trained and then fine-tuned on many different tasks, including sentiment classification.\n",
        "The core part of BERT makes use of the Transformer model, an attention based model that learns contextual relations between words in a text. A Transformer consists of an encoder that produces representations from the input and a decoder that outputs a prediction for a specific task.\n",
        "BERT only uses the encoder, because its goal is to produce a language model.\n",
        "BERT is bidirectional in the sense that it doesn't read an input sequence from one direction word by word, but reads the entire sequence at once, which allows it to learn contextual information of a word from all the words to both of its sides.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "As opposed to retraining all parameters of a pretrained model, a common approach is to fix the majority of a model’s parameters and only train a subset of selected parameters with the new data. For pretrained BERT models, these parameters are typically the ones on the output classification layer and the ones of the intermediate layer norms. It was shown in (Luet al., 2021) that transformer models are even capable of generalizing to different input modalities when pre-trained on language data and only fine-tuning the above-mentioned parameters, which is why finetuning these parameters for our task is a reasonable approach.\n",
        "\n",
        "When fine-tuning a pre-trained model, it may be the case that performance is improved when the pretrained model was trained on data and for a task that is similar to the data and task at hand. For this reason, we used the Twitter roBERTa Base for Sentiment Analysis model as our base for fine-tuning.\n",
        "It is a roBERTa-based model which was trained on approximately 124M English tweets and then fine-tuned for sentiment analysis with the TweetEval benchmark.\n",
        "It classifies tweets into the three categories Positive, Negative, and Neutral. The difference between BERT and roBERTa is that the latter uses an improved masking strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB82PtvCCDC1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "\n",
        "class CustomSentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes_pos, n_classes_neg):\n",
        "        super(CustomSentimentClassifier, self).__init__()\n",
        "        config = AutoConfig.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        config.output_hidden_states = True\n",
        "        self.roberta = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", config=config)\n",
        "        #self.roberta = AutoModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", config=config)\n",
        "        print(self.roberta.config)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        #self.out_pos = nn.Linear(self.roberta.config.hidden_size, n_classes_pos)\n",
        "        #self.out_neg = nn.Linear(self.roberta.config.hidden_size, n_classes_neg)\n",
        "        self.out = nn.Linear(self.roberta.config.hidden_size, n_classes_pos + n_classes_neg)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.n_classes_pos = n_classes_pos\n",
        "        self.n_classes_neg = n_classes_neg\n",
        "\n",
        "         # Freeze the majority of the model's parameters\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last few layers\n",
        "        num_layers_to_unfreeze = 3\n",
        "        for layer in self.roberta.encoder.layer[-num_layers_to_unfreeze:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        #for i, layer in enumerate(self.roberta.encoder.layer):\n",
        "          #if i % 2 == 0:\n",
        "              #for param in layer.parameters():\n",
        "                  #param.requires_grad = False\n",
        "\n",
        "\n",
        "        # Unfreeze the first few layers\n",
        "        #num_layers_to_freeze = 3\n",
        "        #for layer in self.roberta.encoder.layer[:num_layers_to_freeze]:\n",
        "            #for param in layer.parameters():\n",
        "                #param.requires_grad = True\n",
        "\n",
        "        #for param in self.roberta.pooler.parameters():\n",
        "           #param.requires_grad = True\n",
        "        #for param in self.drop.parameters():\n",
        "            #param.requires_grad = True\n",
        "        #for param in self.out_pos.parameters():\n",
        "            #param.requires_grad = True\n",
        "        #for param in self.out_neg.parameters():\n",
        "            #param.requires_grad = True\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "      outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      pooled_output = outputs['pooler_output']\n",
        "      output = self.drop(pooled_output)\n",
        "      #pos_logits = self.out_pos(output)\n",
        "      #neg_logits = self.out_neg(output)\n",
        "      logits = self.out(output)\n",
        "      # Split the logits into positive and negative parts\n",
        "      pos_logits, neg_logits = torch.split(logits, self.n_classes_pos, dim=1)\n",
        "\n",
        "      # Apply softmax separately to the positive and negative parts\n",
        "      #pos_probs = self.softmax(pos_logits)\n",
        "      #neg_probs = self.softmax(neg_logits)\n",
        "    \n",
        "      return pos_logits, neg_logits\n",
        "\n"
      ],
      "metadata": {
        "id": "puJaDXHkBu5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, tweets, pos_labels, neg_labels, tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.pos_labels = pos_labels\n",
        "        self.neg_labels = neg_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if item >= len(self.tweets):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        tweet = str(self.tweets[item])\n",
        "        pos_label = self.pos_labels[item] - 1\n",
        "        neg_label = abs(self.neg_labels[item]) - 1\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"pos_label\": torch.tensor(pos_label, dtype=torch.long),\n",
        "            \"neg_label\": torch.tensor(neg_label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(tweets, pos_labels, neg_labels, tokenizer, max_len, batch_size):\n",
        "    dataset = SentimentDataset(tweets[:500], pos_labels[:500], neg_labels[:500], tokenizer, max_len)\n",
        "    return DataLoader(dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "VWnpnbfmHuCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 240\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "train_data_loader = create_data_loader(train_tweets, train_pos, train_neg, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(val_tweets, val_pos, val_neg, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-5\n",
        "from torch import optim\n",
        "num_classes_pos = 5  # Number of positive sentiment classes\n",
        "num_classes_neg = 5  # Number of negative sentiment classes\n",
        "model = CustomSentimentClassifier(num_classes_pos, num_classes_neg)\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "nm_steps = 3*len(train_data_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGWZ-v3LIATa",
        "outputId": "082487e7-5ba3-471b-bb15-14be4243f554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    #model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    pos_preds_list, neg_preds_list = [], []\n",
        "    pos_labels_list, neg_labels_list = [], []\n",
        "    with tqdm(total=len(data_loader), desc=\"Training\", unit=\"batch\", leave=False) as pbar:\n",
        "      for batch in data_loader:\n",
        "          input_ids = batch[\"input_ids\"].to(device)\n",
        "          attention_mask = batch[\"attention_mask\"].to(device)\n",
        "          pos_labels = batch[\"pos_label\"].to(device)\n",
        "          neg_labels = batch[\"neg_label\"].to(device)\n",
        "\n",
        "    \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "\n",
        "          pos_logits, neg_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "          pos_loss = loss_fn(pos_logits, pos_labels)\n",
        "          neg_loss = loss_fn(neg_logits, neg_labels)\n",
        "          total_batch_loss = pos_loss + neg_loss\n",
        "\n",
        "          total_batch_loss.backward()\n",
        "          optimizer.step()\n",
        "          #lr_scheduler.step()\n",
        "          \n",
        "\n",
        "          total_loss += total_batch_loss.item()\n",
        "\n",
        "          # Apply softmax separately to the positive and negative parts\n",
        "          pos_probs = F.softmax(pos_logits, dim=1)\n",
        "          neg_probs = F.softmax(neg_logits, dim=1)\n",
        "\n",
        "          # Calculate accuracy\n",
        "          _, pos_preds = torch.max(pos_probs, 1)\n",
        "          _, neg_preds = torch.max(neg_probs, 1)\n",
        "          pos_preds = pos_preds + 1  # shift back to original range\n",
        "          neg_preds = (neg_preds + 1) * -1  # shift back to original range\n",
        "          pos_labels = pos_labels + 1  # shift back to original range\n",
        "          neg_labels = (neg_labels + 1) * -1  # shift back to original range\n",
        "\n",
        "          total_correct += (pos_preds == pos_labels).sum().item() + (neg_preds == neg_labels).sum().item()\n",
        "          total_samples += pos_labels.size(0) + neg_labels.size(0)\n",
        "          accuracy = total_correct / total_samples\n",
        "\n",
        "          pbar.update(1)\n",
        "          pbar.set_postfix({\"Loss\": total_loss / (pbar.n + 1), \"Accuracy\": accuracy})\n",
        "          \n",
        "\n",
        "    return total_loss / len(data_loader), total_correct / total_samples\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_accuracy = train_epoch(model, train_data_loader, loss_fn, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f} \")\n",
        "    #Evaluation\n",
        "    model.eval()\n",
        "    val_loss, val_accuracy = train_epoch(model, val_data_loader, loss_fn, optimizer, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f} \")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), f\"/content/gdrive/MyDrive/joint_roberta_sentiment_model.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzFYkKG-KhQZ",
        "outputId": "19d53b21-09c4-431c-cd32-b7927d92641f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Training Loss: 2.1025\n",
            "Training Accuracy: 0.6360 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.7778\n",
            "Validation Accuracy: 0.7070 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3\n",
            "Training Loss: 1.7311\n",
            "Training Accuracy: 0.7030 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.5916\n",
            "Validation Accuracy: 0.7200 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3\n",
            "Training Loss: 1.5979\n",
            "Training Accuracy: 0.6980 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.5044\n",
            "Validation Accuracy: 0.7200 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "model.load_state_dict(torch.load(f\"/content/gdrive/MyDrive/joint_roberta_sentiment_model.pt\"))\n",
        "def test_epoch(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    pos_preds_all = []\n",
        "    neg_preds_all = []\n",
        "    true_pos_labels = []\n",
        "    true_neg_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      with tqdm(total=len(data_loader), desc=\"Training\", unit=\"batch\", leave=False) as pbar:\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pos_labels = batch[\"pos_label\"].to(device)\n",
        "            neg_labels = batch[\"neg_label\"].to(device)\n",
        "\n",
        "            pos_logits, neg_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Apply softmax separately to the positive and negative parts\n",
        "            pos_probs = F.softmax(pos_logits, dim=1)\n",
        "            neg_probs = F.softmax(neg_logits, dim=1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, pos_preds = torch.max(pos_probs, 1)\n",
        "            _, neg_preds = torch.max(neg_probs, 1)\n",
        "\n",
        "            #pos_preds = pos_preds.argmax(dim=1)\n",
        "            #neg_preds = neg_preds.argmax(dim=1)\n",
        "\n",
        "            pos_preds_all.extend(pos_preds.cpu().numpy())\n",
        "            neg_preds_all.extend(neg_preds.cpu().numpy())\n",
        "            true_pos_labels.extend(pos_labels.cpu().numpy())\n",
        "            true_neg_labels.extend(neg_labels.cpu().numpy())\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Converting predictions and true labels back to original range\n",
        "    adjusted_pos_preds = [pred + 1 for pred in pos_preds_all]\n",
        "    adjusted_neg_preds = [-(pred + 1) for pred in neg_preds_all]\n",
        "    adjusted_true_pos_labels = [label + 1 for label in true_pos_labels]\n",
        "    adjusted_true_neg_labels = [-(label + 1) for label in true_neg_labels]\n",
        "\n",
        "    total_correct = sum([pred == label for pred, label in zip(adjusted_pos_preds, adjusted_true_pos_labels)]) + \\\n",
        "                    sum([pred == label for pred, label in zip(adjusted_neg_preds, adjusted_true_neg_labels)])\n",
        "    total_samples = len(adjusted_true_pos_labels) + len(adjusted_true_neg_labels)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    pos_f1_score = f1_score(adjusted_true_pos_labels, adjusted_pos_preds, average='weighted')\n",
        "    neg_f1_score = f1_score(adjusted_true_neg_labels, adjusted_neg_preds, average='weighted')\n",
        "\n",
        "    return accuracy, pos_f1_score, neg_f1_score\n",
        "\n",
        "test_neg = test_neg.astype(int)\n",
        "test_pos = test_pos.astype(int)\n",
        "test_data_loader = create_data_loader(test_tweets, test_pos, test_neg, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "accuracy, pos_f1_score, neg_f1_score = test_epoch(model, test_data_loader, device)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Positive sentiment F1 score: {pos_f1_score:.4f}\")\n",
        "print(f\"Negative sentiment F1 score: {neg_f1_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK5qvJuQQa6p",
        "outputId": "a16d1231-e934-44a8-ca0e-efd2fcd32e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6400\n",
            "Positive sentiment F1 score: 0.7121\n",
            "Negative sentiment F1 score: 0.3787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ebt4fsXNQdgw"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ML4HC",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}